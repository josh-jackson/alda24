---
title: "bends"
execute: 
  echo: true
format: revealjs
editor: visual
---

## Trajectories that bend

```{r, echo = FALSE}
library(tidyverse)
library(lavaan)
library(modelr)
library(lme4)
```

-   Thus far we have been sticking with monotonically increasing trajectories. This is a good assumption given the amount of data often found, along with the simplicity.

-   Often we want to see if trajectories are not straight. Development is not simple so our lines should not be.

-   Need effective strategies for line that bend that also balance tradeoffs with interpretability and overfitting

## Polynomial and Splines

Polynomials (quadratic) level 1: $${Y}_{ij} = \beta_{0j}  + \beta_{1j}(Time_{ij} - \bar{X)} + \beta_{2j}(Time_{ij} - \bar{X)}^2 + \varepsilon_{ij}$$

Level 2: $${\beta}_{0j} = \gamma_{00} +   U_{0j}$$

$${\beta}_{1j} = \gamma_{10} +  U_{1j}$$ 
$${\beta}_{2j} = \gamma_{20} +  U_{2j}$$

## MLM poly example

```{r}
#| code-fold: true
personality <- read.csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/Subject_personality.csv")

ggplot(personality,
   aes(x = neodate, y = neuroticism, group = mapid)) + geom_line()  

```

------------------------------------------------------------------------

```{r}
#| code-fold: true
personality<- personality %>% 
  group_by(mapid) %>%
  arrange(neodate) %>% 
  dplyr::mutate(wave = seq_len(n())) 
```

```{r}
#| code-fold: true
ggplot(personality,
   aes(x = wave, y = neuroticism, group = mapid)) + geom_line()  

```

------------------------------------------------------------------------

```{r}
#| code-fold: true
personality$neodate <- as.Date(personality$neodate, origin = "1900-01-01")

ggplot(personality,
   aes(x = neodate, y = neuroticism, group = mapid)) + geom_line()  


```

------------------------------------------------------------------------

```{r}
#| code-fold: true
# yes this code could be done more efficiently
personality.wide <- personality %>% 
  dplyr::select(mapid, wave, neodate) %>% 
  spread(wave, neodate) 

personality.wide$wave_1 <- personality.wide$'1'
personality.wide$wave_2 <- personality.wide$'2'
personality.wide$wave_3 <- personality.wide$'3'
personality.wide$wave_4 <- personality.wide$'4'
personality.wide$wave_5 <- personality.wide$'5'

personality.wide <- personality.wide %>% 
mutate (w_1 = (wave_1 - wave_1)/365,
          w_2 = (wave_2 - wave_1)/365,
          w_3 = (wave_3 - wave_1)/365,
          w_4 = (wave_4 - wave_1)/365,
        w_5 = (wave_5 - wave_1)/365)

personality.long <- personality.wide %>% 
  dplyr::select(mapid, w_1:w_5) %>% 
  gather(wave, year, -mapid) %>% 
  separate(wave, c('weeks', 'wave' ), sep="_") %>% 
 dplyr::select(-weeks) 

personality.long$wave <-  as.numeric(personality.long$wave)


personality <- personality %>% 
   left_join(personality.long, by = c('mapid', 'wave' )) 

personality.s <- personality %>% 
  group_by(mapid) %>% 
  tally() %>% 
   filter(n >=2) 

 personality <- personality %>% 
   filter(mapid %in% personality.s$mapid)

personality <- personality %>% 
  select(-neodate)
 
personality
```

------------------------------------------------------------------------

```{r}

p1 <- lmer(extraversion ~ year + (year | mapid), data=personality)
summary(p1)
```

------------------------------------------------------------------------

quadratic

```{r, eval = FALSE}
p2 <- lmer(extraversion ~ year + I(year^2) + (1 + year  | mapid), data=personality)
```

I() wont work on difftime objects. Booo

------------------------------------------------------------------------

quadratic

```{r}

personality <- personality %>% 
  mutate(year = as.numeric(year))

p2 <- lmer(extraversion ~ year + I(year^2) + (1 + year | mapid), data=personality)
```

------------------------------------------------------------------------

```{r}
summary(p2)
```

------------------------------------------------------------------------

#### The importance of centering

-   This is an interaction model, where you have a level 1 interaction. As such, centering is important to correctly interpret parameters.

------------------------------------------------------------------------

```{r}
personality <- personality %>% 
  mutate(year.c = year - 3.10)

p3 <- lmer(extraversion ~ year.c + I(year.c^2) + (1 + year.c | mapid), data=personality)
```

------------------------------------------------------------------------

```{r}
summary(p3)
```

------------------------------------------------------------------------

graphically, what does this look like?

```{r}
#| code-fold: true
personality %>% 
  data_grid(year.c = seq(-3.1,10, 1), .model = personality) %>% 
  add_predictions(p3) %>% 
   group_by(year.c) %>% 
  dplyr::summarize(pred = mean(pred)) %>% 
  ggplot(aes(x = year.c, y = pred)) +
  geom_line()

```

------------------------------------------------------------------------

non-centered model

```{r}
#| code-fold: true
personality %>% 
  data_grid(year = seq(0,13, 1), .model = personality) %>% 
  add_predictions(p2) %>% 
   group_by(year) %>% 
  dplyr::summarize(pred = mean(pred)) %>% 
  ggplot(aes(x = year, y = pred)) +
  geom_line()

```

------------------------------------------------------------------------

```{r}
#| code-fold: true
personality %>% 
  data_grid(year.c = seq(-4,10, 1), .model = personality) %>% 
  add_predictions(p3) %>% 
  ggplot(aes(x = year.c, y = pred, group = mapid)) +
  geom_line(alpha = .15)

```

------------------------------------------------------------------------

compare with a linear model

```{r}
anova(p3, p1)
```

## SEM poly example

```{r}
#| code-fold: true

#use alcohol data from before
alcohol <- read.csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/alcohol1_pp.csv")

alcohol.wide <- alcohol %>% 
  dplyr::select(-X, -age_14, -ccoa) %>% 
  pivot_wider(names_from = "age", 
              names_prefix = "alcuse_",
              values_from  = alcuse) 
alcohol.wide
```

------------------------------------------------------------------------

```{r}
#| code-fold: true

model.4 <- '  

i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
s =~ 0*alcuse_14 + 2*alcuse_15 + 4*alcuse_16
q =~ 0*alcuse_14 + 4*alcuse_15 + 16*alcuse_16  

q~~0*q

alcuse_14~~a*alcuse_14
alcuse_15~~a*alcuse_15
alcuse_16~~a*alcuse_16
'

p4 <- growth(model.4, data = alcohol.wide, missing = "ML")

```

## centering in SEM

Because we control the scaling of time via our constraints we do not need to explicitly center time in the same way we did in the MLM model

------------------------------------------------------------------------

```{r}
summary(p4)
```

------------------------------------------------------------------------

Lets use the personality data from the mlm above. First gotta convert into wide.

```{r}
#| code-fold: true
personality2 <- read.csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/Subject_personality.csv")

p.wide<- personality2 %>% 
  group_by(mapid) %>%
  arrange(neodate) %>% 
  dplyr::mutate(wave = seq_len(n())) %>% 
  select(-c(age:neuroticism), -c(openness:gender)) %>% 
  pivot_wider(names_from = "wave", values_from = "extraversion",names_prefix = "extra_")

p.wide
```

------------------------------------------------------------------------

```{r}

model.5 <- '  

i =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 
s =~ 0*extra_1 + 1*extra_2 + 2*extra_3 + 3*extra_4 + 4*extra_5 
q =~ 0*extra_1 + 1*extra_2 + 4*extra_3 + 9*extra_4 + 16*extra_5  

'

p5 <- growth(model.5, data = p.wide, missing = "ML")

```

------------------------------------------------------------------------

```{r}
summary(p5, fit.measures = TRUE, standardize = TRUE)
```

------------------------------------------------------------------------

constrain variances

```{r}

model.6 <- '  

i =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 
s =~ 0*extra_1 + 1*extra_2 + 2*extra_3 + 3*extra_4 + 4*extra_5 
q =~ 0*extra_1 + 1*extra_2 + 4*extra_3 + 9*extra_4 + 16*extra_5  

extra_1 ~~ Q*extra_1
extra_2 ~~ Q*extra_2
extra_3 ~~ Q*extra_3
extra_4 ~~ Q*extra_4
extra_5 ~~ Q*extra_5
'

p6 <- growth(model.6, data = p.wide, missing = "ML")

```

------------------------------------------------------------------------

```{r}
summary(p6, fit.measures = TRUE, standardize = TRUE)
```

------------------------------------------------------------------------

```{r}
head(lavPredict(p6,type="lv"))
```

------------------------------------------------------------------------

```{r}
head(lavPredict(p6,type="ov"))
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
as_tibble(lavPredict(p6,type="ov")) %>% 
  rowid_to_column("ID") %>% 
  pivot_longer(cols = starts_with("extra"), names_to = c(".value", "wave"), names_sep = "_") %>%
dplyr::mutate(wave = as.numeric(wave)) %>% 
ggplot(aes(x = wave, y = extra, group = ID, color = factor(ID))) +
  geom_line(alpha = .2) +  theme(legend.position = "none") 
```

## SEM latent basis

```{r}

#| code-fold: true
model.7 <- '  

i =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 
s =~ 0*extra_1 + extra_2 + extra_3 + extra_4 + 4*extra_5 

'

p7 <- growth(model.7, data = p.wide, missing = "ML")

```

------------------------------------------------------------------------

```{r}
summary(p7, fit.measures = TRUE, standardize = TRUE)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
as_tibble(lavPredict(p7,type="ov")) %>% 
  rowid_to_column("ID") %>% 
  pivot_longer(cols = starts_with("extra"), names_to = c(".value", "wave"), names_sep = "_") %>%
dplyr::mutate(wave = as.numeric(wave)) %>% 
ggplot(aes(x = wave, y = extra, group = ID, color = factor(ID))) +
  geom_line(alpha = .2) +  theme(legend.position = "none") 
```

------------------------------------------------------------------------

## Piecewise

-   Fit more than 1 trajectory

-   Best to use when we have a reason for a qualitative difference at a time point. For example, before your health event you may have a different trajectory than after

-   Time modeled as dummy variables that represent different segments

-   The point of separation is called a knot. You can have as many as you want and these can be pre-specified or let the data specify

------------------------------------------------------------------------

#### two-rate specification

-   The easiest example is to take your time variable and transform it into a Time1 and time2, that represent the different time periods

```{r}
t1 <- tribble(
  ~time, ~t0, ~t1,~t2,~t3,~t4,~t5,
  "time 1", 0, 1,2,2,2,2,
  "time 2", 0, 0,0,1,2,3
)
t1
```

-   Once you hit the knot your value stays the same. For the second curve, until you get to knot you don't have a trajectory.

------------------------------------------------------------------------

#### incremental curves

-   Here the first trajectory keeps going, whereas the second trajectory starts at the position of the knot.

```{r}
t2 <- tribble(
  ~time, ~t0, ~t1,~t2,~t3,~t4,~t5,
  "time 1", 0, 1,2,3,4,5,
  "time 2", 0, 0,0,1,2,3
)
t2
```

------------------------------------------------------------------------

-   The two coding schemes propose the same type of trajectory, the difference is in interpretation.
-   In the first, the two slope coefficients represent the actual slope in the respective time period.
-   In the second, the coefficient for time 2 represents the deviation from the slope in period 1.

## mlm example

level 1:

$${Y}_{ij} = \beta_{0j}  + \beta_{1j}Time1_{ij} + \beta_{2j}Time2_{ij} + \varepsilon_{ij}$$

Level 2: $${\beta}_{0j} = \gamma_{00} +  U_{0j}$$

$${\beta}_{1j} = \gamma_{10} +  U_{1j}$$ $${\beta}_{2j} = \gamma_{20} +  U_{2j}$$

------------------------------------------------------------------------

0 1 2 2 2\
0 0 0 1 2

```{r}

personality$time1 <- dplyr::recode(personality$wave, '1' = 0 , '2' = 1,  '3' = 2, '4' = 2,'5' = 2)      
personality$time2 <- recode(personality$wave, '1' = 0 , '2' = 0,  '3' = 0, '4' = 1,'5' = 2) 


```

------------------------------------------------------------------------

```{r}
p7 <- lmer(extraversion ~ time1 + time2 + (time2 | mapid) , data=personality)
summary(p7)
```

------------------------------------------------------------------------

0 1 3 4 5 (Wave)\
0 0 0 1 2 (same as time 2 previously)

```{r}
p8 <- lmer(extraversion ~ wave + time2 + (time2  | mapid) , data=personality)
summary(p8)
```

## SEM example

0 1 2 2 2\
0 0 0 1 2

```{r}
two.rate <- 'i =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 
s1 =~ 0*extra_1 + 1*extra_2 + 2*extra_3 + 2*extra_4 + 2*extra_5 
s2 =~ 0*extra_1 + 0*extra_2 + 0*extra_3 + 1*extra_4 + 2*extra_5  
'

p8 <- growth(two.rate, data = p.wide, missing = "ML")


```

------------------------------------------------------------------------

```{r}
summary(p8, fit.measures = TRUE, standardize = TRUE)
```

------------------------------------------------------------------------

0 1 2 3 4\
0 0 0 1 2

```{r}
incremental <- 'i =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 
s1 =~ 0*extra_1 + 1*extra_2 + 2*extra_3 + 3*extra_4 + 4*extra_5 
s2 =~ 0*extra_1 + 0*extra_2 + 0*extra_3 + 1*extra_4 + 2*extra_5  
'

p9 <- growth(incremental, data = p.wide, missing = "ML")


```

------------------------------------------------------------------------

```{r}
summary(p9, fit.measures = TRUE, standardize = TRUE)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
as_tibble(lavPredict(p9,type="ov")) %>% 
  rowid_to_column("ID") %>% 
  pivot_longer(cols = starts_with("extra"), names_to = c(".value", "wave"), names_sep = "_") %>%
dplyr::mutate(wave = as.numeric(wave)) %>% 
ggplot(aes(x = wave, y = extra, group = ID, color = factor(ID))) +
  geom_line(alpha = .2) +  theme(legend.position = "none") 

```

------------------------------------------------------------------------

different model, same figure?

```{r}
#| code-fold: true
as_tibble(lavPredict(p8,type="ov")) %>% 
  rowid_to_column("ID") %>% 
  pivot_longer(cols = starts_with("extra"), names_to = c(".value", "wave"), names_sep = "_") %>%
dplyr::mutate(wave = as.numeric(wave)) %>% 
ggplot(aes(x = wave, y = extra, group = ID, color = factor(ID))) +
  geom_line(alpha = .2) +  theme(legend.position = "none") 

```

------------------------------------------------------------------------

## splines + polynomial = polynomial piecewise

$${Y}_{ij} = \beta_{0j}  + \beta_{1j}Time1_{ij} +  \beta_{2j}Time1_{ij}^2 + \beta_{3j}Time2_{ij} + \varepsilon_{ij}$$

Level 2: $${\beta}_{0j} = \gamma_{00} +  U_{0j}$$

$${\beta}_{1j} = \gamma_{10} +  U_{1j}$$ $${\beta}_{2j} = \gamma_{20} +  U_{2j}$$ $${\beta}_{3j} = \gamma_{30} +  U_{3j}$$

------------------------------------------------------------------------

you really should have more waves per piece to model piecewise polynomial, but hey lets try it:

```{r}
#| code-fold: true
two.rate.poly <- 'i =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 
s1 =~ -2*extra_1 + -1*extra_2 + 0*extra_3 + 0*extra_4 + 0*extra_5 
s2 =~ 0*extra_1 + 0*extra_2 + 0*extra_3 + 1*extra_4 + 2*extra_5  
s2poly =~ 0*extra_1 + 0*extra_2 + 0*extra_3 + 1*extra_4 + 4*extra_5 

extra_1 ~~ Q*extra_1
extra_2 ~~ Q*extra_2
extra_3 ~~ Q*extra_3
extra_4 ~~ Q*extra_4
extra_5 ~~ Q*extra_5

s2poly~~0*s2poly

'

p9 <- growth(two.rate.poly, data = p.wide, missing = "ML")


```

```{r}
summary(p9)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
as_tibble(lavPredict(p9,type="ov")) %>% 
  rowid_to_column("ID") %>% 
  pivot_longer(cols = starts_with("extra"), names_to = c(".value", "wave"), names_sep = "_") %>%
dplyr::mutate(wave = as.numeric(wave)) %>% 
ggplot(aes(x = wave, y = extra, group = ID, color = factor(ID))) +
  geom_line(alpha = .2) +  theme(legend.position = "none") 

```

## Discontinuity in level, not in slopes

- The previous models modified time, and thus the trajectory. But level 1 predictors also modify the trajectory. We will cover these in more detail later, but for now... 

```{r}
#| code-fold: true
library(tidyverse)
plot <- function(data, 
                            mapping, 
                            sizes = c(1, 1/4), 
                            linetypes = c(1, 2), 
                            ...) {
  
  ggplot(data, mapping) +
    geom_line(aes(size = model, linetype = model)) +
    geom_text(data = text,
              aes(label = label, hjust = hjust),
              size = 3, parse = T) +
    geom_segment(data = arrow,
                 aes(xend = xend, yend = yend),
                 arrow = arrow(length = unit(0.075, "inches"), type = "closed"),
                 size = 1/4) +
    scale_size_manual(values = sizes) +
    scale_linetype_manual(values = linetypes) +
    scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
    scale_y_continuous(breaks = 0:4 * 0.2 + 1.6, expand = c(0, 0)) +
    coord_cartesian(ylim = c(1.6, 2.4)) +
    theme(legend.position = "none",
          panel.grid = element_blank())
  
}

text <-
  tibble(exper = c(4.5, 4.5, 7.5, 7.5, 1),
         lnw   = c(2.24, 2.2, 1.82, 1.78, 1.62),
         label = c("Common~rate~of~change",
                   "Pre-Post~L1~Event~(gamma[1][italic(i)])",
                   "Elevation~differential",
                   "on~level~1~IV~(gamma[2][italic(i)])",
                   "DV~at~time~zero~(gamma[0][italic(i)])"),
         hjust = c(.5, .5, .5, .5, 0))

arrow <-
  tibble(exper = c(2.85, 5.2, 5.5, 1.7),
         xend  = c(2, 6.8, 3.1, 0.05),
         lnw   = c(2.18, 2.18, 1.8, 1.64),
         yend  = c(1.84, 2.08, 1.9, 1.74))

p1 <-
  tibble(exper = c(0, 3, 3, 10),
         ged   = rep(0:1, each = 2)) %>% 
  tidyr::expand(model = letters[1:2],
         nesting(exper, ged)) %>% 
  mutate(exper2 = if_else(ged == 0, 0, exper - 3)) %>% 
  mutate(lnw = case_when(
    model == "a" ~ 1.75 + 0.04 * exper,
    model == "b" ~ 1.75 + 0.04 * exper + 0.05 * ged),
  model = fct_rev(model)) %>%
  plot(aes(x = exper, y = lnw))+ xlab("time") +ylab("DV")
p1

```


## Splines

```{r}
#| code-fold: true
text <-
  tibble(exper = c(5, 5, 0.5, 0.5, 1),
         lnw   = c(2.24, 2.2, 2, 1.96, 1.62),
         label = c("Slope~differential",
                   "Pre-Post~knot~(gamma[2][italic(i)])",
                   "Rate~of~change",
                   "Pre~knot~(gamma[1][italic(i)])",
                   "DV~at~time~0(gamma[0][italic(i)])"),
         hjust = c(.5, .5, 0, 0, 0))

arrow <-
  tibble(exper = c(5.2, 1.7, 1.7),
         xend  = c(9.1, 1.7, 0.05),
         lnw   = c(2.18, 1.93, 1.64),
         yend  = c(2.15, 1.84, 1.74))

p2 <-
  tibble(exper = c(0, 3, 3, 10),
         ged   = rep(0:1, each = 2)) %>% 
 tidyr::expand(model = letters[1:2],
         nesting(exper, ged)) %>% 
  mutate(postexp = ifelse(exper == 10, 1, 0)) %>% 
  mutate(lnw = case_when(
    model == "a" ~ 1.75 + 0.04 * exper,
    model == "b" ~ 1.75 + 0.04 * exper + 0.15 * postexp),
  model = fct_rev(model)) %>%
  
  plot(aes(x = exper, y = lnw)) +
  annotate(geom = "curve",
           x = 8.5, xend = 8.8,
           y = 2.195, yend = 2.109,
           arrow = arrow(length = unit(0.05, "inches"), type = "closed", ends = "both"),
           size = 1/4, linetype = 2, curvature = -0.85)+ xlab("time") +ylab("DV")
  
p2  
```

## Generalized growth curves

- Often times your DV is not Logistic and 


## Non-linear models



## GAMs

-   Standard longitudinal models are simple to understand (lines!), but fail to capture the ebbs and flows of many time series. Yet there are downsides to polynomial models, as they end up making bad predictions at the ends

- We can do localized regression such as lowess but those are non-parameteric and thus we cannot do standard inferential tests on this  

- GAMs offer a middle ground: they can be fit to complex, nonlinear relationships and make good predictions in these cases, but we are still able to do inferential statistics and understand and explain the underlying structure of our models 


## Generalized additive models

Similar to going from general linear model to generalized linear model. One more generalization. 

$$y \sim ExpoFam(\mu, etc.)$$
$$E(y) = \mu$$

$$g(\mu) = b_0 + f(x_1) + f(x_2) \;...\;+f(x_p)$$

exponential family distribution, and  μ   is still related to the model predictors via a link function. The key difference is that the linear predictor now incorporates smooth functions of at least some (possibly all) features, represented as f(x), and this will allow for nonlinear relationships between the features and the target variable y.


## When to use? 

- Take a look at your scatter plots, are they monotononicly linear? 
- Residuals vs fitted look off? QQ plot looking bent? 
- Transformations help in general but make edge cases even worse? 
- Polynomial not cutting it
- You want to be as flexible as possible

## Simplified GAM

$$y = f(x) + \epsilon$$

choosing a basis, which in technical terms means choosing a space of functions for which (f) is some element of it.

$$y = f(x) + \epsilon = \sum_{j=1}^{d}F_j(x)\beta_j + \epsilon$$
Above, each Fj is a basis function that is the transformed x depending on the type of basis considered, and the b are the corresponding regression coefficients. 

----------

Can think of the basis function as a polynomial with d = 2 (but it could be many other things too)
$$y = f(x) + \epsilon = \sum_{j=1}^{d}F_j(x)\beta_j + \epsilon$$

$$f(x) = b_0 + b_1\cdot x^1 \ldots +b_d\cdot x^d$$
These basis functions become extra variables used to predict, though each of them are often "hidden" behind the overly bland "basis function" 


## Penalized estimation 

- One concern is that we overfit our data and make our lines too wiggly. We can overcome this via penalized estimation, basically balancing minimizing residuals with out of sample prediction. 

- This is what is used in lasso or ridge regression or many other machine learning algorithms. 

- A penalization parameter often called lambda uses cross validation to balance over vs under fitting

## Knots and basis function

- The major decision points are 1) the number of knots we want to use (similar to piecewise models) and 2) the basis function. 

- A simple basis function is to use a cubic polynomial, but the default of many packages are penalized thin plate regression splines, which work quite well. 




